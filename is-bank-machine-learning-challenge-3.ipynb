{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n#df = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\n#create_download_link(df)\n# ↓ ↓ ↓  Yay, download link! ↓ ↓ ↓","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_unique_col_values(df):\n    for column in df:\n        #if df[column].dtypes=='object':\n            print(f'{column}: {df[column].unique()}')\n\ntrain = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/train.csv\")\n\nexpenditures = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/monthly_expenditures.csv\")\n\nexpenditures_copy=expenditures.copy()\n\nexpenditures_copy = expenditures_copy.pivot_table(index=['musteri'], \n            columns=['sektor','tarih'], values=['islem_adedi']).fillna(1).reset_index()\n\nexpenditures = expenditures.pivot_table(index=['musteri'], \n            columns=['sektor','tarih'], values=['aylik_toplam_tutar']).fillna(0).reset_index()\n               \ndf2 = pd.merge(train, expenditures, \"left\", on = \"musteri\")\ndf2.drop(columns=['tarih','meslek_grubu'],inplace=True)#'meslek_grubu'\ndf2 = pd.merge(df2, expenditures_copy, \"left\", on = \"musteri\")\n\n#fill NANs with the most frequent value\n#df2 = df2.apply(lambda x:x.fillna(x.value_counts().index[0]))\n#clip negative values in kidem_suresi\ndf2.loc[df2['kidem_suresi'] < 0, 'kidem_suresi'] = 0\n\n#df2=pd.get_dummies(df2, columns=['egitim','is_durumu','meslek_grubu'],dummy_na=True,drop_first=True)#'meslek_grubu'\n\n#from sklearn.preprocessing import LabelEncoder\n#le=LabelEncoder()\n#df2.egitim=le.fit_transform(df2.egitim)\n#df2.is_durumu=le.fit_transform(df2.is_durumu)\n#df2.meslek_grubu=le.fit_transform(df2.meslek_grubu)#remove this line\n\n\ndf2.drop(columns=['musteri'],inplace=True)#,'egitim_2eb5ddd72c','is_durumu_3773727d6e'\n\n#columns_to_scale=list(df2.loc[ : , df2.columns != 'target'] )\n#columns_to_scale=['yas','kidem_suresi']+(list(df2)[5:161])\n#from sklearn.preprocessing import MinMaxScaler\n#scaler=MinMaxScaler()\n#df2[columns_to_scale]=scaler.fit_transform(df2[columns_to_scale])\n\ndf2=df2.dropna()\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=list(df2)[5:161]\nlist1=x[0:78]\nlist2=x[78:156]\nfor i in range(len(list1)):\n    df2[list1[i]]=df2[list1[i]]/df2[list2[i]]\n\n'''holder=0\ncount=0\nwhile holder<len(list1):\n    row=0\n    for val in df2[list1[holder]]:\n        if val!=0:\n            val=val/(df2.iloc[row][list2[holder]])\n            row+=1\n        else:\n            val=0\n            row+=1\n    #df2.drop(list1[i],inplace=True)\n    #df2.drop(list2[holder],inplace=True)\n    holder+=1\n    count+=1\nprint(count)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df2=pd.get_dummies(df2, columns=['egitim','is_durumu','meslek_grubu'],dummy_na=True,drop_first=True)#'meslek_grubu'\n#df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.drop(df2.iloc[:, 83:161], inplace = True, axis = 1) \ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.drop(columns=['kidem_suresi'],inplace=True)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf2.egitim=le.fit_transform(df2.egitim)\ndf2.is_durumu=le.fit_transform(df2.is_durumu)\n\ncolumns_to_scale=list(df2)[0:3]+list(df2)[4:82]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ndf2[columns_to_scale]=scaler.fit_transform(df2[columns_to_scale])\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2_columns=[]\ntest_df2_columns=[]\nfor col in df2:\n    if col!='target':\n        df2_columns.append(col)\nfor col in test_df2:\n    test_df2_columns.append(col)\n#print(df2_columns)\n#print(test_df2_columns)\n\nholder=0\ncount=0\nwhile holder<len(df2_columns):\n    #\n    if df2_columns[holder]!=test_df2_columns[holder]:\n        count+=1\n        print(count)\n        print(test_df2_columns[holder])\n    holder+=1","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check if there is missing values\n#missing=df2[df2.isnull().any(axis=1)]\n#print(missing)\nprint_unique_col_values(df2)\n#print(df2.target.value_counts())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = df2.drop('target',axis='columns')\n#y = df2.target\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df2,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#handling the imbalanced dataset by UPSAMPLING THE MINORITY CLASS\n\nfrom sklearn.utils import resample\n# Separate majority and minority classes\ndf_majority = train[train.target==0]\ndf_minority = train[train.target==1]\n\n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=45827,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\ntrain=df_upsampled\n\n# Display new class counts\ndf_upsampled.target.value_counts()\n# 1    576\n# 0    576\n# Name: balance, dtype: int64","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#handling the imbalanced dataset by DOWNSAMPLING THE MAJORITY CLASS\n\nfrom sklearn.utils import resample\n# Separate majority and minority classes\ndf_majority = train[train.target==0]\ndf_minority = train[train.target==1]\n \n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=1929,     # to match minority class\n                                 random_state=123) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\ntrain=df_downsampled\n \n# Display new class counts\ndf_downsampled.target.value_counts()\n# 1    49\n# 0    49\n# Name: balance, dtype: int64","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\nfrom pylab import rcParams\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nnp.random.seed(27)\nrcParams['figure.figsize'] = 10, 6\nwarnings.filterwarnings('ignore')\nsns.set(style=\"darkgrid\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_model_report(y_actual, y_predicted):\n    print(\"Accuracy = \" , accuracy_score(y_actual, y_predicted))\n    print(\"Precision = \" ,precision_score(y_actual, y_predicted))\n    print(\"Recall = \" ,recall_score(y_actual, y_predicted))\n    print(\"F1 Score = \" ,f1_score(y_actual, y_predicted))\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_auc_roc_curve(clf, X_test):\n    y_pred_proba = clf.predict(X_test)#[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'target'\nX = df2.loc[:, df2.columns!=target]\ny = df2.loc[:, df2.columns==target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#ax = sns.countplot(x=target, data=df2)\n#print(df2[target].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(100* (2431/float(df2.shape[0])))\nprint(100* (57264/float(df2.shape[0])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1,class_weight={0:0.05,1:0.95})\n\nsvm.fit(X_train,y_train)\n\nprint('Support Vector Machine Score : ',svm.score(X_test,y_test))\n\n#from sklearn import svm\n#from sklearn.svm import SVC\n#clf = svm.SVC()\n#clf.fit(X_train, y_train)\n#SVC()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clf = RandomForestClassifier().fit(X_train, y_train)\ny_Test_Pred = svm.predict(X_test)\npd.crosstab(y_Test_Pred, y_test[target], rownames=['Predicted'], colnames=['Actual'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_Test_Pred)\ngenerate_auc_roc_curve(clf, X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# class_weight='balanced'","metadata":{}},{"cell_type":"code","source":"unique_classes = list(df2['target'].unique())\nprint(unique_classes)\nout_dict = {}\nfor classes in unique_classes:\n    out_dict[classes] = df2.shape[0]/((df2.loc[df2['target'] == classes].shape[0])\n                                     *len(unique_classes))\nprint(out_dict)\nprint (X_train.shape, y_train.shape)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\nfrom sklearn.utils import class_weight\nclass_weight.compute_class_weight('balanced', np.unique(y_train), y_train[target])\ny_Test_Pred = clf.predict(X_test)\npd.crosstab(y_Test_Pred, y_test[target], rownames=['Predicted'], colnames=['Actual'])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_Test_Pred)\ngenerate_auc_roc_curve(clf, X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = np.linspace(0.05, 0.95, 20)\ngsc = GridSearchCV(\n    estimator=model(),\n    param_grid={\n        'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n    },\n    scoring='f1',\n    cv=5\n)\n\ngrid_result = gsc.fit(X_train, y_train)\nprint(\"Best parameters : %s\" % grid_result.best_params_)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_out = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'],\n                       'weight': weights })\ndata_out.plot(x='weight')\ndata_out","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(**grid_result.best_params_).fit(X_train, y_train)\ny_Test_Pred = clf.predict(X_test)\npd.crosstab(y_Test_Pred, y_test[target], rownames=['Predicted'], colnames=['Actual'])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_Test_Pred)\ngenerate_auc_roc_curve(clf, X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SMOTE","metadata":{}},{"cell_type":"code","source":"unique, count = np.unique(y_train, return_counts=True)\ny_train_dict_value_count = { k:v for (k,v) in zip(unique, count)}\ny_train_dict_value_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sm = SMOTE(random_state=42, sampling_strategy=1.0)\nX_train, y_train = sm.fit_sample(X_train, y_train)\nunique, count = np.unique(y_train, return_counts=True)\ny_train_smote_value_count = { k:v for (k,v) in zip(unique, count)}\ny_train_smote_value_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression().fit(X_train, y_train)\ny_Test_Pred = clf.predict(X_test)\npd.crosstab(y_Test_Pred, y_test[target], rownames=['Predicted'], colnames=['Actual'])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_Test_Pred)\ngenerate_auc_roc_curve(clf, X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = np.linspace(0.005, 0.25, 10)\nweights","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = make_pipeline(\n    SMOTE(),\n    LogisticRegression()\n)\n\nweights = np.linspace(0.5, 0.75, 10)\n\ngsc = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'smote__sampling_strategy': weights\n    },\n    scoring='f1',\n    cv=3\n)\ngrid_result = gsc.fit(X_train, y_train)\n\nprint(\"Best parameters : %s\" % grid_result.best_params_)\nweight_f1_score_df = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],\n                                   'weight': weights })\nweight_f1_score_df.plot(x='weight')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nsorted(sklearn.metrics.SCORERS.keys())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = make_pipeline(\n    SMOTE(sampling_strategy=0.25),\n    LogisticRegression()\n)\n\npipe.fit(X_train, y_train) \ny_Test_Pred = (pipe.predict_proba(X_test)[:,1]>=0.25).astype(int) \npd.crosstab(y_Test_Pred, y_test[target], rownames=['Predicted'], colnames=['Actual'])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_Test_Pred)\ngenerate_auc_roc_curve(clf, X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###############################################################","metadata":{}},{"cell_type":"code","source":"#preparing dataset\n#X = df2.drop('target',axis='columns')\n#y = df2['target']\n\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10,stratify=y)\n\n# Feature Scaling\n#from sklearn.preprocessing import StandardScaler\n\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)\n\n#training the random forest classfier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(random_state=42,n_estimators=100,class_weight='balanced').fit(X_train, y_train)\n#y_pred = classifier.predict_proba(X_test)\ny_pred_01=classifier.predict(X_test)\n#evaluation\nfrom sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,classification_report\nprint('auc_roc_score = ',roc_auc_score(y_test, y_pred_01))\nprint('accuracy = ',accuracy_score(y_test, y_pred_01))\nprint(classification_report(y_test, y_pred_01))\nconfusion_matrix(y_test, y_pred_01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/test.csv\")\n\ntest_expenditures = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/monthly_expenditures.csv\")\n#test_expenditures = test_expenditures.drop(columns=['islem_adedi'])\ntest_expenditures = test_expenditures.pivot_table(index=['musteri'], \n            columns=['sektor','tarih'], values='aylik_toplam_tutar','islem_adedi').fillna(0).reset_index()\n                \ntest_df2 = pd.merge(test, test_expenditures, \"left\", on = \"musteri\")\ntest_df2.drop(columns=['tarih','meslek_grubu'],inplace=True)#'meslek_grubu'\n#fill NANs with the most frequent value\ntest_df2 = test_df2.apply(lambda x:x.fillna(x.value_counts().index[0]))\n#clip negative values in kidem_suresi\ntest_df2.loc[test_df2['kidem_suresi'] < 0, 'kidem_suresi'] = 0\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntest_df2.egitim=le.fit_transform(test_df2.egitim)\ntest_df2.is_durumu=le.fit_transform(test_df2.is_durumu)\n#test_df2.meslek_grubu=le.fit_transform(test_df2.meslek_grubu)#remove this line\n\ntest_musteri_column=test_df2.pop('musteri')\n#test_df2.drop(columns=['musteri'],inplace=True)\n\n#columns_to_scale=list(test_df2.loc[ : , test_df2.columns != 'target'] )\n#from sklearn.preprocessing import MinMaxScaler\n#scaler=MinMaxScaler()\n#test_df2[columns_to_scale]=scaler.fit_transform(test_df2[columns_to_scale])\n\n#test_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df2","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yp = clf.predict(test_df2)\nyp=yp.tolist()\nyp","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfor element in yp[:,1]:\n    if element > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('0: ',yp.count(0),'\\n1: ',yp.count(1))\n# class 1 count is over 16000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nwith open('result.csv', 'w') as f:\n    writer = csv.writer(f)\n    writer.writerows(zip(test_musteri_column, yp))\n    \nsubmission = pd.read_csv(\"./result.csv\",names=['musteri','target'])\nsubmission=submission.groupby('musteri').target.agg(lambda x : x.mode()[0]).reset_index()\n#*******************************************#\nsubmission.set_index('musteri').to_csv('LogisticRegression_DOWNSAMPLED_without_meslek_grubu.csv')\nsubmission","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **NOW WE WILL BUILD A DEEP LEARNING MODEL**","metadata":{}},{"cell_type":"code","source":"X_train = train.drop('target',axis='columns')\ny_train = train['target']\nX_test=test.drop('target',axis='columns')\ny_test = test['target']\n\n#from imblearn.under_sampling import NearMiss\n# Implementing Undersampling for Handling Imbalanced \n#nm = NearMiss()\n#X_train,y_train=nm.fit_sample(X_train,y_train)\n#X_train.shape,y_train.shape\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train['target']\ny_test = y_test['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(X_train.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, input_shape=(81,), activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n'''\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(81,)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n#opt = keras.optimizers.Adam(learning_rate=0.01)\n'''\n#keras.optimizers.Adam(lr=1e-3)\nmodel.compile(optimizer='sgd',\n              loss='binary_crossentropy',\n              metrics=[tf.keras.metrics.AUC()])\n\nmodel.fit(X_train, y_train, class_weight={0:1,1:23.56},epochs=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class weighted neural network on an imbalanced classification dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n# prepare train and test dataset\n#def prepare_data():\n# generate 2d classification dataset\n#X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n#n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n# split into train and test\n#n_train = 5000\n#trainX, testX = X[:n_train, :], X[n_train:, :]\n#trainy, testy = y[:n_train], y[n_train:]\n#return trainX, trainy, testX, testy\n\n# define the neural network model\ndef define_model(n_input):\n    # define model\n    model = Sequential()\n    # define first hidden layer and visible layer\n    model.add(Dense(10, input_dim=n_input, activation='relu', kernel_initializer='he_uniform'))\n    # define output layer\n    model.add(Dense(1, activation='sigmoid'))\n    # define loss and optimizer\n    model.compile(loss='binary_crossentropy', optimizer='Adam')\n    return model\n\n# prepare dataset\n#trainX, trainy, testX, testy = prepare_data()\n# get the model\nn_input = X_train.shape[1]\nmodel = define_model(n_input)\n# fit model\nweights = {0:1, 1:23.56}\nhistory = model.fit(X_train, y_train, class_weight=weights, epochs=50, verbose=0)\n# evaluate model\nyhat = model.predict(X_test)\nscore = roc_auc_score(y_test, yhat)\nprint('ROC AUC: %.3f' % score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yp=model.predict_proba(X_test)\ny_pred = []\nfor element in yp:\n    if element > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ny_pred=pd.Series(y_pred)\n#y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(y_pred, y_test, rownames=['Predicted'], colnames=['Actual'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_model_report(y_test, y_pred)\ngenerate_auc_roc_curve(model, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy=model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##############################################################","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/test.csv\")\n\ntest_expenditures = pd.read_csv(\"../input/turkiye-is-bankas-machine-learning-challenge-3/monthly_expenditures.csv\")\n\ntest_expenditures_copy=test_expenditures.copy()\ntest_expenditures_copy = test_expenditures_copy.pivot_table(index=['musteri'], \n            columns=['sektor','tarih'], values=['islem_adedi']).fillna(1).reset_index()\n\n\ntest_expenditures = test_expenditures.pivot_table(index=['musteri'], \n            columns=['sektor','tarih'], values=['aylik_toplam_tutar']).fillna(0).reset_index()\n                \ntest_df2 = pd.merge(test, test_expenditures, \"left\", on = \"musteri\")\ntest_df2.drop(columns=['tarih','meslek_grubu'],inplace=True)#'meslek_grubu'\n\ntest_df2 = pd.merge(test_df2, test_expenditures_copy, \"left\", on = \"musteri\")\n\n#fill NANs with the most frequent value\ntest_df2 = test_df2.apply(lambda x:x.fillna(x.value_counts().index[0]))\n#clip negative values in kidem_suresi\ntest_df2.loc[test_df2['kidem_suresi'] < 0, 'kidem_suresi'] = 0\n\n#test_df2=pd.get_dummies(test_df2, columns=['egitim','is_durumu',],dummy_na=True,drop_first=True)#'meslek_grubu'\n\n#from sklearn.preprocessing import LabelEncoder\n#le=LabelEncoder()\n#test_df2.egitim=le.fit_transform(test_df2.egitim)\n#test_df2.is_durumu=le.fit_transform(test_df2.is_durumu)\n#test_df2.meslek_grubu=le.fit_transform(test_df2.meslek_grubu)#remove this line\n\ntest_musteri_column=test_df2.pop('musteri')\n#test_df2.drop(columns=['musteri'],inplace=True)\n\n#test_df2.insert(158,'egitim_2eb5ddd72c',0)\n#test_df2.insert(164,'is_durumu_3773727d6e',0)\n\n#columns_to_scale=list(test_df2.loc[ : , test_df2.columns != 'target'] )\n#from sklearn.preprocessing import MinMaxScaler\n#scaler=MinMaxScaler()\n#test_df2[columns_to_scale]=scaler.fit_transform(test_df2[columns_to_scale])\n#test_df2.dropna(inplace=True)\ntest_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=list(test_df2)[4:160]\nlist1=x[0:78]\nlist2=x[78:156]\nfor i in range(len(list1)):\n    test_df2[list1[i]]=test_df2[list1[i]]/test_df2[list2[i]]\n\n'''holder=0\ncount=0\nwhile holder<len(list1):\n    row=0\n    for val in test_df2[list1[holder]]:\n        if val!=0:\n            val=val/(test_df2.iloc[row][list2[holder]])\n            row+=1\n        else:\n            val=0\n            row+=1\n    #df2.drop(list1[i],inplace=True)\n    #df2.drop(list2[holder],inplace=True)\n    holder+=1\n    count+=1\nprint(count)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_df2=pd.get_dummies(test_df2, columns=['egitim','is_durumu','meslek_grubu'],dummy_na=True,drop_first=True)#'meslek_grubu'\ntest_df2.drop(test_df2.iloc[:, 82:160], inplace = True, axis = 1)\n\n#test_df2.insert(80,'egitim_2eb5ddd72c',0)\n#test_df2.insert(86,'is_durumu_3773727d6e',0)\n\ntest_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df2.drop(columns=['kidem_suresi'],inplace=True)\ncolumns_to_scale=list(test_df2)[0:3]+list(test_df2)[3:81]\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntest_df2.egitim=le.fit_transform(test_df2.egitim)\ntest_df2.is_durumu=le.fit_transform(test_df2.is_durumu)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ntest_df2[columns_to_scale]=scaler.fit_transform(test_df2[columns_to_scale])\ntest_df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yp = model.predict(test_df2)\n#yp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfor element in yp:\n    if element > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ntype(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_zeros=y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_ones=y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_list=[]\nfor i in range(40000):\n    if list_of_zeros[i]==0 and list_of_ones[i]==0:\n        final_list.append(list_of_zeros[i])\n    elif list_of_zeros[i]==0 and list_of_ones[i]==1:\n        final_list.append(list_of_zeros[i])\n    elif list_of_zeros[i]==1 and list_of_ones[i]==0:\n        final_list.append(list_of_ones[i])\n    else:\n        final_list.append(list_of_ones[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_list.count(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_count=str(' class0: '+str(y_pred.count(0))+'  class1: '+str(y_pred.count(1)))\nprint(class_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=pd.Series(y_pred)\ntype(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample= str('sample(droupout 0.5)= ' +str(int(len(df2.target)/2))+class_count+' accuracy= '+str(accuracy[1]))\nlist_of_samples.append(sample)\nlist_of_samples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(test_musteri_column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nwith open('result.csv', 'w') as f:\n    writer = csv.writer(f)\n    writer.writerows(zip(test_musteri_column, y_pred))\n    \nsubmission = pd.read_csv(\"./result.csv\",names=['musteri','target'])\n#submission=submission.groupby('musteri').target.agg(lambda x : x.mode()[0]).reset_index()\n#*******************************************#\nsubmission.set_index('musteri').to_csv('ANN_class_weighted.csv')\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_samples=['sample= 3000 class0: 22591  class1: 17409 accuracy= 0.7513161301612854',\n 'sample= 4000 class0: 22434  class1: 17566 accuracy= 0.751573920249939',\n 'sample= 5000 class0: 22349  class1: 17651 accuracy= 0.7583276033401489']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}